{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proyecto Segundo Parcial\n",
    "## Clasificacion supervisada del riesgo de inundacion por parroquia (Provincia del Guayas, Ecuador)\n",
    "\n",
    "**Carrera/Asignatura:** Ciencia de Datos e Inteligencia Artificial  \n",
    "**Tipo de problema:** Clasificacion supervisada (no regresion)  \n",
    "**Unidad territorial:** Parroquia\n",
    "\n",
    "Este cuaderno documenta de forma tecnica y trazable el flujo completo del proyecto:\n",
    "1. Importacion de datos con referencia de fuentes oficiales.\n",
    "2. Limpieza y control de calidad del dataset.\n",
    "3. Construccion de la variable objetivo de riesgo (sin usar etiquetas predefinidas).\n",
    "4. Entrenamiento de modelos requeridos (RL, DT, ensamble).\n",
    "5. Optimizacion con GridSearchCV.\n",
    "6. Evaluacion comparativa con precision, recall, F1 y ROC-AUC.\n",
    "7. Integracion de resultados con el GeoJSON usado por la aplicacion Flask.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Alcance, restricciones y criterio tecnico\n",
    "\n",
    "Este proyecto se apega a las restricciones solicitadas:\n",
    "- Se trabaja con datos de entidades oficiales del Ecuador y una base historica consolidada del proyecto.\n",
    "- No se usan datasets con etiqueta de riesgo final ya predefinida.\n",
    "- La etiqueta objetivo se construye con criterio tecnico reproducible.\n",
    "- Se implementan al menos: Regresion Logistica, Arbol de Decision y Ensamble.\n",
    "- Se aplica GridSearchCV para optimizar al menos un modelo.\n",
    "- La metrica prioritaria para gestion de riesgo es **recall** (minimizar falsos negativos en zonas con riesgo alto).\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    ConfusionMatrixDisplay,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    ")\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "try:\n",
    "    import seaborn as sns\n",
    "    HAS_SNS = True\n",
    "except Exception:\n",
    "    HAS_SNS = False\n",
    "\n",
    "try:\n",
    "    from IPython.display import display\n",
    "except Exception:\n",
    "    def display(x):\n",
    "        print(x)\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', 200)\n",
    "pd.set_option('display.width', 200)\n",
    "\n",
    "if HAS_SNS:\n",
    "    sns.set_theme(style='whitegrid', palette='Set2')\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "ROOT = Path.cwd().resolve()\n",
    "if ROOT.name != 'entregables':\n",
    "    ROOT = ROOT / 'entregables'\n",
    "\n",
    "DATA_RAW = ROOT / 'data' / 'raw'\n",
    "OUTPUTS = ROOT / 'outputs'\n",
    "APP_DATA = ROOT / 'app' / 'data'\n",
    "\n",
    "historical_path = DATA_RAW / 'dataset_proyecto.csv'\n",
    "full_dataset_path = OUTPUTS / 'dataset_guayas_oficial_completo.csv'\n",
    "predictions_path = OUTPUTS / 'predicciones_parroquias.csv'\n",
    "metrics_path = OUTPUTS / 'resumen_metricas_modelos.csv'\n",
    "source_registry_path = OUTPUTS / 'fuentes_y_metodologia_oficial.json'\n",
    "geojson_path = APP_DATA / 'parroquias_riesgo.geojson'\n",
    "\n",
    "for p in [historical_path, full_dataset_path, predictions_path, metrics_path, source_registry_path, geojson_path]:\n",
    "    print(f'{p.name}: {p.exists()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Importacion y referencia de fuentes oficiales\n",
    "\n",
    "Las fuentes usadas por el pipeline quedan registradas en `outputs/fuentes_y_metodologia_oficial.json`.\n",
    "\n",
    "### Fuentes\n",
    "- INEC DPA Parroquias (ArcGIS REST): geometria y codigos oficiales de parroquia.\n",
    "- INEC Censo 2022 MANLOC: variables socio-territoriales (poblacion, hogares, viviendas, estructura demografica).\n",
    "- IGM DTM WMS: variables topograficas (altitud y derivadas de pendiente/rango altitudinal).\n",
    "- Base historica consolidada INAMHI + SNGRE (`data/raw/dataset_proyecto.csv`): series climaticas y eventos de inundacion historicos.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "with source_registry_path.open('r', encoding='utf-8') as f:\n",
    "    source_registry = json.load(f)\n",
    "\n",
    "sources_df = pd.DataFrame([\n",
    "    {'fuente': k, 'referencia': v}\n",
    "    for k, v in source_registry.get('fuentes_oficiales', {}).items()\n",
    "])\n",
    "\n",
    "variables_df = pd.DataFrame([\n",
    "    {'grupo': grupo, 'variables': ', '.join(vars_list)}\n",
    "    for grupo, vars_list in source_registry.get('variables_integradas', {}).items()\n",
    "])\n",
    "\n",
    "print('Fuentes oficiales:')\n",
    "display(sources_df)\n",
    "print('Variables integradas por grupo:')\n",
    "display(variables_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Carga de datos\n",
    "\n",
    "Se cargan dos datasets principales:\n",
    "- `dataset_proyecto.csv`: base historica mensual por parroquia para construir etiquetas y variables climaticas.\n",
    "- `dataset_guayas_oficial_completo.csv`: dataset final integrado por parroquia (clima + topografia + socio-territorial).\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_hist_raw = pd.read_csv(historical_path)\n",
    "df_full = pd.read_csv(full_dataset_path)\n",
    "\n",
    "print('Historico:', df_hist_raw.shape)\n",
    "print('Integrado final:', df_full.shape)\n",
    "\n",
    "print('Columnas historico:')\n",
    "print(df_hist_raw.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Limpieza del dataset historico (requisito del proyecto)\n",
    "\n",
    "En esta seccion se documenta limpieza con pasos explicitos:\n",
    "1. Normalizacion de nombres de columnas y codigos.\n",
    "2. Correccion de tipos numericos.\n",
    "3. Correccion de formato de fechas (`anio`, `mes` -> `fecha_periodo`).\n",
    "4. Revision y manejo de duplicados.\n",
    "5. Revision de valores faltantes.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 4.1 Estandarizacion minima de nombres\n",
    "rename_map = {\n",
    "    'Codigo': 'codigo',\n",
    "    'Código': 'codigo',\n",
    "    'Nombre de provincia': 'provincia',\n",
    "    'Nombre de canton': 'canton',\n",
    "    'Nombre de parroquia': 'parroquia',\n",
    "    'Población': 'poblacion',\n",
    "    'Superficie de la parroquia (km2)': 'superficie_km2',\n",
    "    'Densidad Poblacional': 'densidad_poblacional',\n",
    "    'Latitud': 'latitud',\n",
    "    'Longitud': 'longitud',\n",
    "    'Precipitacion_Anual': 'precipitacion_anual',\n",
    "    'Cerca_Rio': 'cerca_rio',\n",
    "}\n",
    "\n",
    "df_hist = df_hist_raw.rename(columns=rename_map).copy()\n",
    "\n",
    "def normalize_code(value: object) -> str:\n",
    "    digits = ''.join(ch for ch in str(value) if ch.isdigit())\n",
    "    return digits.lstrip('0')\n",
    "\n",
    "if 'codigo' in df_hist.columns:\n",
    "    df_hist['codigo'] = df_hist['codigo'].map(normalize_code)\n",
    "\n",
    "# 4.2 Tipos numericos\n",
    "numeric_cols = [\n",
    "    'poblacion', 'superficie_km2', 'densidad_poblacional', 'latitud', 'longitud',\n",
    "    'precipitacion_anual', 'anio', 'mes', 'precipitacion_mm', 'temp_media_c',\n",
    "    'humedad_relativa', 'inundacion', 'cerca_rio'\n",
    "]\n",
    "for col in numeric_cols:\n",
    "    if col in df_hist.columns:\n",
    "        df_hist[col] = pd.to_numeric(df_hist[col], errors='coerce')\n",
    "\n",
    "# 4.3 Fecha de periodo\n",
    "if {'anio', 'mes'}.issubset(df_hist.columns):\n",
    "    df_hist['anio'] = df_hist['anio'].astype('Int64')\n",
    "    df_hist['mes'] = df_hist['mes'].astype('Int64')\n",
    "    valid_ym = df_hist['anio'].notna() & df_hist['mes'].between(1, 12)\n",
    "    df_hist.loc[valid_ym, 'fecha_periodo'] = pd.to_datetime(\n",
    "        dict(year=df_hist.loc[valid_ym, 'anio'].astype(int), month=df_hist.loc[valid_ym, 'mes'].astype(int), day=1),\n",
    "        errors='coerce'\n",
    "    )\n",
    "\n",
    "# 4.4 Duplicados: se evalua por codigo-anio-mes\n",
    "subset_dups = [c for c in ['codigo', 'anio', 'mes'] if c in df_hist.columns]\n",
    "dups_count = int(df_hist.duplicated(subset=subset_dups).sum()) if subset_dups else 0\n",
    "\n",
    "# 4.5 Faltantes\n",
    "missing_report = df_hist.isna().sum().sort_values(ascending=False).head(15)\n",
    "\n",
    "print('Filas historico (despues de limpieza):', len(df_hist))\n",
    "print('Duplicados en llave codigo-anio-mes:', dups_count)\n",
    "print('Top faltantes:')\n",
    "display(missing_report.to_frame('nulos'))\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "if dups_count > 0:\n",
    "    df_hist = df_hist.drop_duplicates(subset=subset_dups, keep='first').copy()\n",
    "    print('Se eliminaron duplicados. Filas finales historico:', len(df_hist))\n",
    "else:\n",
    "    print('No se detectaron duplicados en codigo-anio-mes.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Construccion y justificacion de la variable objetivo\n",
    "\n",
    "No se usa una etiqueta de riesgo final predefinida. Se construye una etiqueta binaria a partir del historico de inundacion:\n",
    "\n",
    "- `eventos_inundacion`: suma de eventos por parroquia.\n",
    "- `total_periodos`: numero de periodos observados por parroquia.\n",
    "- `tasa_inundacion_historica = eventos_inundacion / total_periodos`.\n",
    "- Umbral de alto riesgo: percentil 66 de la tasa historica.\n",
    "- Etiqueta final para entrenamiento: `target_alto_riesgo` (1 = alto riesgo, 0 = no alto).\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "if 'inundacion' not in df_hist.columns:\n",
    "    raise ValueError(\"La columna 'inundacion' es obligatoria para construir la etiqueta objetivo.\")\n",
    "\n",
    "df_hist['inundacion'] = pd.to_numeric(df_hist['inundacion'], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "labels_df = (\n",
    "    df_hist.groupby('codigo', as_index=False)\n",
    "    .agg(eventos_inundacion=('inundacion', 'sum'), total_periodos=('inundacion', 'count'))\n",
    ")\n",
    "labels_df['tasa_inundacion_historica'] = labels_df['eventos_inundacion'] / labels_df['total_periodos'].replace(0, np.nan)\n",
    "labels_df['tasa_inundacion_historica'] = labels_df['tasa_inundacion_historica'].fillna(0)\n",
    "\n",
    "threshold = float(labels_df['tasa_inundacion_historica'].quantile(0.66))\n",
    "labels_df['target_alto_riesgo'] = (labels_df['tasa_inundacion_historica'] >= threshold).astype(int)\n",
    "\n",
    "print(f'Umbral percentil 66: {threshold:.4f}')\n",
    "print(labels_df['target_alto_riesgo'].value_counts().rename({0: 'No alto', 1: 'Alto'}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Integracion de variables climaticas + topograficas + socio-territoriales\n",
    "\n",
    "El dataset integrado `outputs/dataset_guayas_oficial_completo.csv` contiene variables de los tres grupos:\n",
    "\n",
    "### Climaticas\n",
    "- `precipitacion_mensual_prom_mm`\n",
    "- `precipitacion_mensual_p95_mm`\n",
    "- `precipitacion_anual_prom_mm`\n",
    "- `temperatura_media_prom_c`\n",
    "- `humedad_relativa_prom`\n",
    "- `cerca_rio_prom`\n",
    "\n",
    "### Topograficas\n",
    "- `altitud_igm_m`\n",
    "- `pendiente_igm` (**derivada**)\n",
    "- `rango_altitud_igm_m` (**derivada**)\n",
    "\n",
    "### Socio-territoriales\n",
    "- `poblacion_2022`, `hogares_2022`, `viviendas_2022`\n",
    "- `densidad_poblacional_2022`\n",
    "- `pct_urbana_2022`\n",
    "- `indice_compacidad` (**derivada**), entre otras.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "required_features = [\n",
    "    'precipitacion_mensual_prom_mm', 'precipitacion_mensual_p95_mm', 'precipitacion_anual_prom_mm',\n",
    "    'temperatura_media_prom_c', 'humedad_relativa_prom', 'cerca_rio_prom',\n",
    "    'altitud_igm_m', 'pendiente_igm', 'rango_altitud_igm_m',\n",
    "    'poblacion_2022', 'hogares_2022', 'viviendas_2022', 'densidad_poblacional_2022',\n",
    "    'pct_urbana_2022', 'indice_compacidad'\n",
    "]\n",
    "\n",
    "missing_required = [c for c in required_features if c not in df_full.columns]\n",
    "print('Variables requeridas faltantes:', missing_required)\n",
    "\n",
    "na_report = df_full[required_features].isna().sum().sort_values(ascending=False)\n",
    "print('Nulos por variable (integrado final):')\n",
    "display(na_report.to_frame('nulos'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Analisis exploratorio visual de datos (EDA)\n",
    "\n",
    "Esta seccion incorpora visualizaciones para comprender mejor la estructura de la data, su calidad,\n",
    "patrones climaticos, relaciones entre variables y comportamiento espacial del riesgo.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 7.1 Calidad de datos: porcentaje de faltantes por variable\n",
    "hist_missing_pct = (df_hist.isna().mean() * 100).sort_values(ascending=False)\n",
    "full_missing_pct = (df_full.isna().mean() * 100).sort_values(ascending=False)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "axes[0].barh(hist_missing_pct.head(15).index[::-1], hist_missing_pct.head(15).values[::-1], color='#4C78A8')\n",
    "axes[0].set_title('Top 15 faltantes - Dataset historico (%)')\n",
    "axes[0].set_xlabel('% faltantes')\n",
    "\n",
    "axes[1].barh(full_missing_pct.head(15).index[::-1], full_missing_pct.head(15).values[::-1], color='#F58518')\n",
    "axes[1].set_title('Top 15 faltantes - Dataset integrado (%)')\n",
    "axes[1].set_xlabel('% faltantes')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 7.2 Distribuciones climaticas clave\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "if HAS_SNS:\n",
    "    sns.histplot(df_hist['precipitacion_mm'].dropna(), kde=True, ax=axes[0], color='#2A9D8F')\n",
    "    sns.histplot(df_hist['temp_media_c'].dropna(), kde=True, ax=axes[1], color='#E76F51')\n",
    "    sns.histplot(df_hist['humedad_relativa'].dropna(), kde=True, ax=axes[2], color='#264653')\n",
    "else:\n",
    "    axes[0].hist(df_hist['precipitacion_mm'].dropna(), bins=30, color='#2A9D8F', alpha=0.8)\n",
    "    axes[1].hist(df_hist['temp_media_c'].dropna(), bins=30, color='#E76F51', alpha=0.8)\n",
    "    axes[2].hist(df_hist['humedad_relativa'].dropna(), bins=30, color='#264653', alpha=0.8)\n",
    "\n",
    "axes[0].set_title('Distribucion de precipitacion mensual (mm)')\n",
    "axes[1].set_title('Distribucion de temperatura media (C)')\n",
    "axes[2].set_title('Distribucion de humedad relativa (%)')\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set_ylabel('Frecuencia')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('Resumen descriptivo climatico:')\n",
    "display(df_hist[['precipitacion_mm', 'temp_media_c', 'humedad_relativa']].describe().T)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 7.3 Estacionalidad: precipitacion e inundacion por mes\n",
    "monthly = (\n",
    "    df_hist.groupby('mes', as_index=False)\n",
    "    .agg(\n",
    "        precipitacion_prom_mm=('precipitacion_mm', 'mean'),\n",
    "        tasa_inundacion=('inundacion', 'mean')\n",
    "    )\n",
    "    .sort_values('mes')\n",
    ")\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(12, 5))\n",
    "ax1.plot(monthly['mes'], monthly['precipitacion_prom_mm'], marker='o', color='#1D3557', linewidth=2)\n",
    "ax1.set_xlabel('Mes')\n",
    "ax1.set_ylabel('Precipitacion promedio (mm)', color='#1D3557')\n",
    "ax1.tick_params(axis='y', labelcolor='#1D3557')\n",
    "ax1.set_xticks(range(1, 13))\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(monthly['mes'], monthly['tasa_inundacion'], marker='s', color='#E63946', linewidth=2)\n",
    "ax2.set_ylabel('Tasa historica de inundacion', color='#E63946')\n",
    "ax2.tick_params(axis='y', labelcolor='#E63946')\n",
    "\n",
    "plt.title('Comportamiento estacional de precipitacion e inundacion')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 7.4 Serie temporal: tendencia de precipitacion y eventos\n",
    "if 'fecha_periodo' in df_hist.columns:\n",
    "    ts = (\n",
    "        df_hist.dropna(subset=['fecha_periodo'])\n",
    "        .groupby('fecha_periodo', as_index=False)\n",
    "        .agg(\n",
    "            precipitacion_prom_mm=('precipitacion_mm', 'mean'),\n",
    "            eventos_inundacion=('inundacion', 'sum'),\n",
    "        )\n",
    "        .sort_values('fecha_periodo')\n",
    "    )\n",
    "\n",
    "    ts['precip_mm_ma3'] = ts['precipitacion_prom_mm'].rolling(window=3, min_periods=1).mean()\n",
    "\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(14, 8), sharex=True)\n",
    "\n",
    "    axes[0].plot(ts['fecha_periodo'], ts['precipitacion_prom_mm'], color='#457B9D', alpha=0.55, label='Promedio mensual')\n",
    "    axes[0].plot(ts['fecha_periodo'], ts['precip_mm_ma3'], color='#1D3557', linewidth=2, label='Media movil 3 meses')\n",
    "    axes[0].set_title('Serie temporal de precipitacion promedio')\n",
    "    axes[0].set_ylabel('mm')\n",
    "    axes[0].legend()\n",
    "\n",
    "    axes[1].plot(ts['fecha_periodo'], ts['eventos_inundacion'], color='#E63946', linewidth=1.8)\n",
    "    axes[1].set_title('Serie temporal de eventos de inundacion (conteo)')\n",
    "    axes[1].set_ylabel('Eventos')\n",
    "    axes[1].set_xlabel('Periodo')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print('No existe fecha_periodo para la serie temporal.')\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 7.5 Correlaciones entre variables numericas y target\n",
    "corr_base_cols = [\n",
    "    'precipitacion_mensual_prom_mm', 'precipitacion_mensual_p95_mm', 'precipitacion_anual_prom_mm',\n",
    "    'temperatura_media_prom_c', 'humedad_relativa_prom', 'cerca_rio_prom',\n",
    "    'altitud_igm_m', 'pendiente_igm', 'rango_altitud_igm_m',\n",
    "    'poblacion_2022', 'densidad_poblacional_2022', 'pct_urbana_2022',\n",
    "    'indice_compacidad', 'target_alto_riesgo'\n",
    "]\n",
    "\n",
    "corr_df = df_full[corr_base_cols].copy()\n",
    "corr_df = corr_df[corr_df['target_alto_riesgo'].notna()].copy()\n",
    "\n",
    "corr = corr_df.corr(numeric_only=True)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "if HAS_SNS:\n",
    "    sns.heatmap(corr, cmap='RdBu_r', center=0, annot=False, linewidths=0.5)\n",
    "else:\n",
    "    plt.imshow(corr, cmap='RdBu_r', aspect='auto')\n",
    "    plt.colorbar()\n",
    "    plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)\n",
    "    plt.yticks(range(len(corr.index)), corr.index)\n",
    "\n",
    "plt.title('Matriz de correlacion (variables numericas)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "target_corr = corr['target_alto_riesgo'].drop('target_alto_riesgo').sort_values(key=lambda s: np.abs(s), ascending=False)\n",
    "print('Variables mas asociadas (correlacion absoluta) con target_alto_riesgo:')\n",
    "display(target_corr.head(10).to_frame('correlacion'))\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 7.6 Distribucion de riesgo y analisis por canton\n",
    "risk_col = 'riesgo_categoria' if 'riesgo_categoria' in df_full.columns else None\n",
    "if risk_col is None:\n",
    "    raise ValueError('No existe riesgo_categoria en el dataset integrado.')\n",
    "\n",
    "risk_order = ['Bajo', 'Medio', 'Alto']\n",
    "risk_counts = df_full[risk_col].value_counts().reindex(risk_order)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "axes[0].bar(risk_counts.index, risk_counts.values, color=['#2A9D8F', '#E9C46A', '#E76F51'])\n",
    "axes[0].set_title('Cantidad de parroquias por categoria de riesgo')\n",
    "axes[0].set_ylabel('Numero de parroquias')\n",
    "\n",
    "canton_risk = pd.crosstab(df_full['canton'], df_full[risk_col])\n",
    "canton_risk = canton_risk.reindex(columns=risk_order, fill_value=0)\n",
    "top_cantons = canton_risk.sum(axis=1).sort_values(ascending=False).head(12).index\n",
    "canton_plot = canton_risk.loc[top_cantons]\n",
    "\n",
    "canton_plot.plot(kind='bar', stacked=True, ax=axes[1], color=['#2A9D8F', '#E9C46A', '#E76F51'])\n",
    "axes[1].set_title('Composicion de riesgo por canton (top 12)')\n",
    "axes[1].set_ylabel('Numero de parroquias')\n",
    "axes[1].set_xlabel('Canton')\n",
    "axes[1].tick_params(axis='x', rotation=75)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 7.7 Visualizacion espacial basica (dispersion georreferenciada)\n",
    "color_map = {'Bajo': '#2A9D8F', 'Medio': '#E9C46A', 'Alto': '#E76F51'}\n",
    "plot_df = df_full.dropna(subset=['latitud', 'longitud']).copy()\n",
    "plot_df['color'] = plot_df['riesgo_categoria'].map(color_map).fillna('#6C757D')\n",
    "\n",
    "size_base = plot_df['poblacion_2022'].fillna(plot_df['poblacion_2022'].median())\n",
    "size_scaled = 20 + 180 * (size_base - size_base.min()) / (size_base.max() - size_base.min() + 1e-9)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(\n",
    "    plot_df['longitud'],\n",
    "    plot_df['latitud'],\n",
    "    c=plot_df['color'],\n",
    "    s=size_scaled,\n",
    "    alpha=0.75,\n",
    "    edgecolors='black',\n",
    "    linewidths=0.4,\n",
    ")\n",
    "plt.title('Parroquias de Guayas: ubicacion, riesgo y tamano poblacional')\n",
    "plt.xlabel('Longitud')\n",
    "plt.ylabel('Latitud')\n",
    "\n",
    "legend_handles = [\n",
    "    plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=clr, markeredgecolor='black', markersize=10, label=lbl)\n",
    "    for lbl, clr in color_map.items()\n",
    "]\n",
    "plt.legend(handles=legend_handles, title='Riesgo', loc='best')\n",
    "plt.grid(alpha=0.25)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hallazgos sugeridos para interpretar al ejecutar el notebook\n",
    "\n",
    "Al correr esta seccion, documenta puntualmente en tu informe:\n",
    "- Meses con mayor precipitacion promedio y su relacion con la tasa de inundacion.\n",
    "- Variables con mayor correlacion con `target_alto_riesgo`.\n",
    "- Cantones con mayor concentracion de parroquias en riesgo medio/alto.\n",
    "- Patrones espaciales observados en la dispersion georreferenciada.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8) Preparacion para modelado y balance de clases\n",
    "\n",
    "Se entrena solo con parroquias que tienen etiqueta historica (`target_alto_riesgo` no nulo).\n",
    "\n",
    "Luego se reserva un conjunto de prueba estratificado. Se reporta balance de clases para transparencia metodologica.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "feature_cols = [\n",
    "    'precipitacion_mensual_prom_mm', 'precipitacion_mensual_p95_mm', 'precipitacion_anual_prom_mm',\n",
    "    'temperatura_media_prom_c', 'humedad_relativa_prom', 'cerca_rio_prom',\n",
    "    'altitud_igm_m', 'pendiente_igm', 'rango_altitud_igm_m',\n",
    "    'superficie_km2', 'shape_length', 'latitud', 'longitud',\n",
    "    'poblacion_2022', 'hogares_2022', 'viviendas_2022',\n",
    "    'edad_promedio_2022', 'pct_mujeres_2022', 'pct_urbana_2022',\n",
    "    'densidad_poblacional_2022', 'personas_por_hogar_2022', 'viviendas_por_km2_2022',\n",
    "    'indice_compacidad'\n",
    "]\n",
    "\n",
    "model_df = df_full.copy()\n",
    "\n",
    "labeled_df = model_df[model_df['target_alto_riesgo'].notna()].copy()\n",
    "unlabeled_df = model_df[model_df['target_alto_riesgo'].isna()].copy()\n",
    "\n",
    "y = labeled_df['target_alto_riesgo'].astype(int)\n",
    "X = labeled_df[feature_cols]\n",
    "\n",
    "print('Parroquias oficiales Guayas:', len(model_df))\n",
    "print('Parroquias con historial para entrenamiento:', len(labeled_df))\n",
    "print('Parroquias sin historial (a predecir):', len(unlabeled_df))\n",
    "print('Balance de clases (entrenamiento):')\n",
    "display(y.value_counts().rename({0: 'No alto', 1: 'Alto'}).to_frame('conteo'))\n",
    "display((y.value_counts(normalize=True) * 100).round(2).rename({0: 'No alto', 1: 'Alto'}).to_frame('porcentaje'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9) Modelos requeridos por la rubrica\n",
    "\n",
    "Se implementan:\n",
    "1. Regresion Logistica (modelo base).\n",
    "2. Arbol de Decision.\n",
    "3. Ensamble (votacion blanda) que combina RL + DT + RF.\n",
    "4. Optimizacion con GridSearchCV sobre Arbol de Decision (metrica objetivo: recall).\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "pre_scaled = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\n",
    "            'num',\n",
    "            Pipeline([\n",
    "                ('imputer', SimpleImputer(strategy='median')),\n",
    "                ('scaler', StandardScaler()),\n",
    "            ]),\n",
    "            feature_cols,\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "pre_unscaled = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\n",
    "            'num',\n",
    "            Pipeline([\n",
    "                ('imputer', SimpleImputer(strategy='median')),\n",
    "            ]),\n",
    "            feature_cols,\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "lr = Pipeline([\n",
    "    ('preprocess', pre_scaled),\n",
    "    ('clf', LogisticRegression(max_iter=2000, class_weight='balanced', random_state=42)),\n",
    "])\n",
    "\n",
    "dt = Pipeline([\n",
    "    ('preprocess', pre_unscaled),\n",
    "    ('clf', DecisionTreeClassifier(random_state=42, class_weight='balanced', min_samples_leaf=2)),\n",
    "])\n",
    "\n",
    "rf = Pipeline([\n",
    "    ('preprocess', pre_unscaled),\n",
    "    ('clf', RandomForestClassifier(\n",
    "        n_estimators=300,\n",
    "        random_state=42,\n",
    "        class_weight='balanced_subsample',\n",
    "        min_samples_leaf=2,\n",
    "    )),\n",
    "])\n",
    "\n",
    "ensemble = VotingClassifier(\n",
    "    estimators=[('lr', lr), ('dt', dt), ('rf', rf)],\n",
    "    voting='soft'\n",
    ")\n",
    "\n",
    "dt_grid = GridSearchCV(\n",
    "    estimator=dt,\n",
    "    param_grid={\n",
    "        'clf__criterion': ['gini', 'entropy'],\n",
    "        'clf__max_depth': [3, 5, 8, None],\n",
    "        'clf__min_samples_split': [2, 5, 10],\n",
    "        'clf__min_samples_leaf': [1, 2, 4],\n",
    "    },\n",
    "    scoring='recall',\n",
    "    cv=5,\n",
    "    n_jobs=1,\n",
    "    refit=True,\n",
    ")\n",
    "\n",
    "lr.fit(X_train, y_train)\n",
    "dt.fit(X_train, y_train)\n",
    "ensemble.fit(X_train, y_train)\n",
    "dt_grid.fit(X_train, y_train)\n",
    "dt_opt = dt_grid.best_estimator_\n",
    "\n",
    "print('Mejores hiperparametros DT (GridSearchCV):')\n",
    "print(dt_grid.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10) Evaluacion comparativa (precision, recall, F1, ROC-AUC)\n",
    "\n",
    "En gestion de riesgo de inundacion, la metrica prioritaria es **recall** de la clase de alto riesgo,\n",
    "porque penaliza con mayor severidad los falsos negativos (parroquias realmente riesgosas clasificadas como no riesgosas).\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "def eval_model(name: str, model, x_test: pd.DataFrame, y_test: pd.Series) -> dict:\n",
    "    y_pred = model.predict(x_test)\n",
    "    y_score = model.predict_proba(x_test)[:, 1]\n",
    "    return {\n",
    "        'modelo': name,\n",
    "        'precision': round(precision_score(y_test, y_pred, zero_division=0), 4),\n",
    "        'recall': round(recall_score(y_test, y_pred, zero_division=0), 4),\n",
    "        'f1': round(f1_score(y_test, y_pred, zero_division=0), 4),\n",
    "        'roc_auc': round(roc_auc_score(y_test, y_score), 4),\n",
    "    }\n",
    "\n",
    "metrics_table = pd.DataFrame([\n",
    "    eval_model('Regresion Logistica (Base)', lr, X_test, y_test),\n",
    "    eval_model('Arbol de Decision', dt, X_test, y_test),\n",
    "    eval_model('Ensamble RL+DT+RF', ensemble, X_test, y_test),\n",
    "    eval_model('Arbol de Decision Optimizado (GridSearchCV)', dt_opt, X_test, y_test),\n",
    "]).sort_values(['recall', 'f1', 'roc_auc'], ascending=False).reset_index(drop=True)\n",
    "\n",
    "display(metrics_table)\n",
    "\n",
    "best_model_name = metrics_table.iloc[0, 0]\n",
    "print('Modelo seleccionado por prioridad recall/f1/auc:', best_model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Curvas ROC comparativas\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "models_for_roc = {\n",
    "    'RL': lr,\n",
    "    'DT': dt,\n",
    "    'Ensamble': ensemble,\n",
    "    'DT Opt': dt_opt,\n",
    "}\n",
    "\n",
    "for label, model in models_for_roc.items():\n",
    "    scores = model.predict_proba(X_test)[:, 1]\n",
    "    fpr, tpr, _ = roc_curve(y_test, scores)\n",
    "    auc_value = roc_auc_score(y_test, scores)\n",
    "    plt.plot(fpr, tpr, label=f'{label} (AUC={auc_value:.3f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], '--', color='gray')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Curvas ROC de modelos de clasificacion')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Matriz de confusion del mejor modelo\n",
    "best_model = {\n",
    "    'Regresion Logistica (Base)': lr,\n",
    "    'Arbol de Decision': dt,\n",
    "    'Ensamble RL+DT+RF': ensemble,\n",
    "    'Arbol de Decision Optimizado (GridSearchCV)': dt_opt,\n",
    "}[best_model_name]\n",
    "\n",
    "ConfusionMatrixDisplay.from_estimator(best_model, X_test, y_test, cmap='Blues')\n",
    "plt.title(f'Matriz de confusion - {best_model_name}')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11) Prediccion final por parroquia y categorias de riesgo\n",
    "\n",
    "Se aplica el mejor modelo a las 54 parroquias oficiales de Guayas.\n",
    "La categoria de riesgo final se obtiene por terciles de la probabilidad de inundacion:\n",
    "- Bajo: probabilidad < P33\n",
    "- Medio: P33 <= probabilidad < P66\n",
    "- Alto: probabilidad >= P66\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "all_probs = best_model.predict_proba(model_df[feature_cols])[:, 1]\n",
    "model_df['probabilidad_inundacion_calc'] = all_probs\n",
    "\n",
    "q_low = float(model_df['probabilidad_inundacion_calc'].quantile(0.33))\n",
    "q_high = float(model_df['probabilidad_inundacion_calc'].quantile(0.66))\n",
    "if q_low == q_high:\n",
    "    q_low, q_high = 0.33, 0.66\n",
    "\n",
    "def risk_label(prob: float) -> str:\n",
    "    if prob >= q_high:\n",
    "        return 'Alto'\n",
    "    if prob >= q_low:\n",
    "        return 'Medio'\n",
    "    return 'Bajo'\n",
    "\n",
    "model_df['riesgo_categoria_calc'] = model_df['probabilidad_inundacion_calc'].apply(risk_label)\n",
    "\n",
    "print(f'Umbral bajo/medio (P33): {q_low:.4f}')\n",
    "print(f'Umbral medio/alto (P66): {q_high:.4f}')\n",
    "print('Distribucion de categorias calculadas:')\n",
    "print(model_df['riesgo_categoria_calc'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Control de cobertura completa y faltantes\n",
    "coverage_cols = ['codigo', 'parroquia', 'canton', 'provincia', 'probabilidad_inundacion', 'riesgo_categoria']\n",
    "\n",
    "df_exported_pred = pd.read_csv(predictions_path)\n",
    "missing_prob = int(df_exported_pred['probabilidad_inundacion'].isna().sum())\n",
    "missing_risk = int(df_exported_pred['riesgo_categoria'].isna().sum())\n",
    "\n",
    "print('Predicciones exportadas:', len(df_exported_pred))\n",
    "print('Parroquias unicas:', df_exported_pred['codigo'].nunique())\n",
    "print('Sin probabilidad:', missing_prob)\n",
    "print('Sin categoria de riesgo:', missing_risk)\n",
    "\n",
    "display(df_exported_pred[coverage_cols].head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12) Integracion geoespacial para la aplicacion web (Flask + Leaflet)\n",
    "\n",
    "Se verifica que el `GeoJSON` consumido por la app contenga propiedades de riesgo y probabilidad por parroquia.\n",
    "Esto valida el emparejamiento entre geometria y prediccion mediante codigo de parroquia.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "with geojson_path.open('r', encoding='utf-8') as f:\n",
    "    geo = json.load(f)\n",
    "\n",
    "features = geo.get('features', [])\n",
    "props_df = pd.DataFrame([ft.get('properties', {}) for ft in features])\n",
    "\n",
    "required_geo_fields = ['codigo', 'parroquia', 'canton', 'provincia', 'riesgo', 'probabilidad']\n",
    "missing_geo_fields = [c for c in required_geo_fields if c not in props_df.columns]\n",
    "\n",
    "print('Features GeoJSON:', len(features))\n",
    "print('Campos requeridos faltantes en GeoJSON:', missing_geo_fields)\n",
    "if not missing_geo_fields:\n",
    "    print('Sin dato de riesgo:', int(props_df['riesgo'].isna().sum()))\n",
    "    print('Sin dato de probabilidad:', int(props_df['probabilidad'].isna().sum()))\n",
    "\n",
    "display(props_df[required_geo_fields].head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13) Resultados finales y contraste con salida oficial del pipeline\n",
    "\n",
    "Para evitar divergencias entre notebook y despliegue, se comparan las metricas obtenidas en este cuaderno\n",
    "contra `outputs/resumen_metricas_modelos.csv` generado por el script oficial del proyecto.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "metrics_exported = pd.read_csv(metrics_path)\n",
    "print('Metricas exportadas por pipeline:')\n",
    "display(metrics_exported)\n",
    "\n",
    "print('Metricas recalculadas en notebook:')\n",
    "display(metrics_table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14) Conclusiones tecnicas\n",
    "\n",
    "1. Se integraron variables climaticas, topograficas y socio-territoriales con trazabilidad de fuente.\n",
    "2. La variable objetivo fue construida desde eventos historicos, sin usar etiquetas de riesgo predefinidas.\n",
    "3. Se entrenaron y compararon los modelos requeridos por la rubrica (RL, DT, ensamble, DT optimizado).\n",
    "4. Recall se uso como criterio prioritario para seleccion del modelo por el contexto de gestion del riesgo.\n",
    "5. Se verifico cobertura completa de las parroquias oficiales de Guayas en prediccion y GeoJSON.\n",
    "\n",
    "### Nota metodologica\n",
    "La robustez del proyecto depende de mantener actualizados los insumos oficiales y reentrenar el pipeline\n",
    "cuando se incorporen nuevos periodos climaticos o nuevos registros oficiales de eventos adversos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15) Reproducibilidad\n",
    "\n",
    "Para regenerar artefactos del proyecto desde cero en este repositorio:\n",
    "\n",
    "```bash\n",
    "python3 ml/train_and_prepare.py\n",
    "```\n",
    "\n",
    "Archivos clave generados:\n",
    "- `outputs/dataset_guayas_oficial_completo.csv`\n",
    "- `outputs/predicciones_parroquias.csv`\n",
    "- `outputs/resumen_metricas_modelos.csv`\n",
    "- `app/data/parroquias_riesgo.geojson`\n",
    "- `app/data/predicciones_parroquias.csv`\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}