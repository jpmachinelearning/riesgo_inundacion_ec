{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Proyecto: Clasificacion Supervisada de Riesgo de Inundacion por Parroquia (Guayas)\n",
        "\n",
        "## Objetivo\n",
        "Disenar e implementar un modelo supervisado que clasifique el riesgo de inundacion por parroquia,\n",
        "cumpliendo las restricciones del proyecto:\n",
        "\n",
        "- uso de datos reales de fuentes oficiales del Ecuador,\n",
        "- sin etiquetas de riesgo predefinidas por terceros,\n",
        "- comparacion de Regresion Logistica, Arbol de Decision y Ensamble,\n",
        "- optimizacion con GridSearchCV,\n",
        "- evaluacion con Precision, Recall, F1 y ROC-AUC.\n",
        "\n",
        "## Alcance de este notebook\n",
        "Este cuaderno esta alineado con el pipeline del repositorio (`ml/train_and_prepare.py`) y documenta:\n",
        "\n",
        "1. limpieza y control de calidad,\n",
        "2. construccion de etiqueta objetivo supervisada,\n",
        "3. ingenieria de variables (incluye variable derivada/transformada),\n",
        "4. entrenamiento y optimizacion,\n",
        "5. validacion de cobertura total de parroquias de Guayas,\n",
        "6. coherencia con artefactos de la aplicacion web.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Preparacion del entorno\n",
        "\n",
        "Este notebook funciona en local o Colab si el repositorio esta clonado.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import json\n",
        "import subprocess\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import (\n",
        "    RocCurveDisplay,\n",
        "    classification_report,\n",
        "    f1_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    roc_auc_score,\n",
        ")\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "\n",
        "def find_project_root() -> Path:\n",
        "    candidates = [\n",
        "        Path.cwd(),\n",
        "        Path.cwd().parent,\n",
        "        Path('/content/entregables'),\n",
        "        Path('/content'),\n",
        "    ]\n",
        "    for candidate in candidates:\n",
        "        if (candidate / 'ml' / 'train_and_prepare.py').exists():\n",
        "            return candidate.resolve()\n",
        "    raise FileNotFoundError('No se encontro la raiz del proyecto con ml/train_and_prepare.py')\n",
        "\n",
        "\n",
        "ROOT = find_project_root()\n",
        "DATA_RAW = ROOT / 'data' / 'raw'\n",
        "OUTPUTS = ROOT / 'outputs'\n",
        "APP_DATA = ROOT / 'app' / 'data'\n",
        "\n",
        "print('ROOT:', ROOT)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Fuentes oficiales y trazabilidad\n",
        "\n",
        "Las fuentes oficiales y el criterio de etiqueta se consolidan en el archivo:\n",
        "\n",
        "- `outputs/fuentes_y_metodologia_oficial.json`\n",
        "\n",
        "Si aun no existe, se generara al ejecutar el pipeline oficial.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "source_registry_path = OUTPUTS / 'fuentes_y_metodologia_oficial.json'\n",
        "\n",
        "if source_registry_path.exists():\n",
        "    source_registry = json.loads(source_registry_path.read_text(encoding='utf-8'))\n",
        "    source_registry\n",
        "else:\n",
        "    print('Aun no existe fuentes_y_metodologia_oficial.json. Se generara en la seccion de pipeline.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Carga y limpieza del dataset historico base\n",
        "\n",
        "Se usa `data/raw/dataset_proyecto.csv` para construir la etiqueta supervisada a partir de eventos historicos.\n",
        "\n",
        "Controles aplicados:\n",
        "\n",
        "- normalizacion de nombres de columna,\n",
        "- tipado numerico,\n",
        "- parseo de fecha (`anio`, `mes`),\n",
        "- eliminacion de duplicados,\n",
        "- reporte de faltantes,\n",
        "- chequeo de balance de la variable `inundacion`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "historical_path = DATA_RAW / 'dataset_proyecto.csv'\n",
        "if not historical_path.exists():\n",
        "    raise FileNotFoundError(f'No existe {historical_path}')\n",
        "\n",
        "raw_df = pd.read_csv(historical_path)\n",
        "\n",
        "rename_map = {\n",
        "    'Codigo': 'codigo',\n",
        "    'CÃ³digo': 'codigo',\n",
        "    'Nombre de provincia': 'provincia',\n",
        "    'Nombre de canton': 'canton',\n",
        "    'Nombre de parroquia': 'parroquia',\n",
        "    'anio': 'anio',\n",
        "    'mes': 'mes',\n",
        "    'inundacion': 'inundacion',\n",
        "}\n",
        "\n",
        "df_hist = raw_df.rename(columns=rename_map).copy()\n",
        "\n",
        "for col in ['codigo', 'anio', 'mes', 'inundacion']:\n",
        "    if col in df_hist.columns:\n",
        "        df_hist[col] = pd.to_numeric(df_hist[col], errors='coerce')\n",
        "\n",
        "before = len(df_hist)\n",
        "df_hist = df_hist.drop_duplicates().copy()\n",
        "after = len(df_hist)\n",
        "\n",
        "if {'anio', 'mes'}.issubset(df_hist.columns):\n",
        "    df_hist['fecha'] = pd.to_datetime(\n",
        "        dict(\n",
        "            year=df_hist['anio'].astype('Int64'),\n",
        "            month=df_hist['mes'].astype('Int64'),\n",
        "            day=1,\n",
        "        ),\n",
        "        errors='coerce',\n",
        "    )\n",
        "\n",
        "print(f'Registros antes de duplicados: {before}')\n",
        "print(f'Registros despues de duplicados: {after}')\n",
        "print()\n",
        "print('Faltantes (top 12):')\n",
        "print(df_hist.isna().sum().sort_values(ascending=False).head(12))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if 'inundacion' not in df_hist.columns:\n",
        "    raise ValueError('No existe la columna inundacion en dataset_proyecto.csv')\n",
        "\n",
        "class_count = df_hist['inundacion'].value_counts(dropna=False).sort_index()\n",
        "class_share = df_hist['inundacion'].value_counts(normalize=True, dropna=False).sort_index()\n",
        "\n",
        "print('Conteo de clases:')\n",
        "print(class_count)\n",
        "print()\n",
        "print('Proporcion de clases:')\n",
        "print(class_share)\n",
        "\n",
        "ax = class_count.plot(kind='bar', color=['#2ea66f', '#df4f43'], title='Balance de inundacion historica')\n",
        "ax.set_xlabel('Clase inundacion')\n",
        "ax.set_ylabel('Frecuencia')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Construccion de la variable objetivo supervisada\n",
        "\n",
        "Se construye una etiqueta parroquial `target_alto_riesgo` desde la tasa historica de inundacion:\n",
        "\n",
        "- `tasa_inundacion_historica = eventos_inundacion / total_periodos`\n",
        "- umbral tecnico: percentil 66 de la tasa historica.\n",
        "\n",
        "Esto cumple la restriccion de no usar etiquetas predefinidas de riesgo.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def normalize_code(value: object) -> str:\n",
        "    digits = ''.join(ch for ch in str(value) if ch.isdigit())\n",
        "    digits = digits.lstrip('0')\n",
        "    return digits\n",
        "\n",
        "\n",
        "def safe_div(num: pd.Series, den: pd.Series) -> pd.Series:\n",
        "    den_safe = den.replace({0: np.nan})\n",
        "    out = num / den_safe\n",
        "    return out.replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "\n",
        "if 'codigo' not in df_hist.columns:\n",
        "    raise ValueError('No existe codigo/Codigo en dataset_proyecto.csv')\n",
        "\n",
        "hist_label = df_hist.copy()\n",
        "hist_label['codigo'] = hist_label['codigo'].map(normalize_code)\n",
        "hist_label['inundacion'] = pd.to_numeric(hist_label['inundacion'], errors='coerce').fillna(0).astype(int)\n",
        "\n",
        "labels_df = hist_label.groupby('codigo', as_index=False).agg(\n",
        "    eventos_inundacion=('inundacion', 'sum'),\n",
        "    total_periodos=('inundacion', 'count'),\n",
        ")\n",
        "labels_df['tasa_inundacion_historica'] = safe_div(\n",
        "    labels_df['eventos_inundacion'], labels_df['total_periodos']\n",
        ").fillna(0)\n",
        "\n",
        "threshold = float(labels_df['tasa_inundacion_historica'].quantile(0.66))\n",
        "labels_df['target_alto_riesgo'] = (\n",
        "    labels_df['tasa_inundacion_historica'] >= threshold\n",
        ").astype(int)\n",
        "\n",
        "print('Parroquias con historial:', len(labels_df))\n",
        "print('Umbral (percentil 66):', round(threshold, 6))\n",
        "labels_df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Ejecucion del pipeline oficial del repositorio\n",
        "\n",
        "Se ejecuta `ml/train_and_prepare.py` para:\n",
        "\n",
        "- descargar datos oficiales faltantes (INEC MANLOC),\n",
        "- integrar geometria parroquial oficial,\n",
        "- entrenar modelos (RL, DT, ensamble, DT optimizado),\n",
        "- generar predicciones para todas las parroquias de Guayas,\n",
        "- exportar archivos para la app Flask.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "script_path = ROOT / 'ml' / 'train_and_prepare.py'\n",
        "if not script_path.exists():\n",
        "    raise FileNotFoundError(script_path)\n",
        "\n",
        "print('Ejecutando:', script_path)\n",
        "subprocess.run([sys.executable, str(script_path)], check=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) Verificacion de cobertura completa en Guayas\n",
        "\n",
        "Se valida que la salida final cubra todas las parroquias oficiales de Guayas y sin faltantes de riesgo/probabilidad.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pred_path = OUTPUTS / 'predicciones_parroquias.csv'\n",
        "full_dataset_path = OUTPUTS / 'dataset_guayas_oficial_completo.csv'\n",
        "missing_pred_path = OUTPUTS / 'parroquias_guayas_sin_historial_predichas.csv'\n",
        "\n",
        "pred_df = pd.read_csv(pred_path, dtype={'codigo': str})\n",
        "full_df = pd.read_csv(full_dataset_path, dtype={'codigo': str})\n",
        "missing_pred_df = pd.read_csv(missing_pred_path, dtype={'codigo': str})\n",
        "\n",
        "pred_df['codigo'] = pred_df['codigo'].str.zfill(6)\n",
        "full_df['codigo'] = full_df['codigo'].str.zfill(6)\n",
        "\n",
        "print('Parroquias oficiales de Guayas:', full_df['codigo'].nunique())\n",
        "print('Parroquias predichas:', pred_df['codigo'].nunique())\n",
        "print('Probabilidades faltantes:', int(pred_df['probabilidad_inundacion'].isna().sum()))\n",
        "print('Riesgos faltantes:', int(pred_df['riesgo_categoria'].isna().sum()))\n",
        "print('Parroquias sin historial (predichas):', len(missing_pred_df))\n",
        "\n",
        "pred_df[['codigo', 'provincia', 'canton', 'parroquia', 'probabilidad_inundacion', 'riesgo_categoria']].head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7) Ingenieria de variables para modelado\n",
        "\n",
        "Variables usadas (coherentes con `ml/train_and_prepare.py`):\n",
        "\n",
        "- superficie y forma: `superficie_km2`, `shape_length`, `indice_compacidad`\n",
        "- localizacion: `latitud`, `longitud`\n",
        "- demografia oficial INEC 2022: `poblacion_2022`, `hogares_2022`, `viviendas_2022`\n",
        "- composicion: `edad_promedio_2022`, `pct_mujeres_2022`, `pct_urbana_2022`\n",
        "- derivadas: `densidad_poblacional_2022`, `personas_por_hogar_2022`, `viviendas_por_km2_2022`\n",
        "\n",
        "Adicionalmente se muestra una transformacion derivada en notebook:\n",
        "\n",
        "- `log_densidad_poblacional_2022 = log1p(densidad_poblacional_2022)`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_df = full_df.copy()\n",
        "\n",
        "model_df['log_densidad_poblacional_2022'] = np.log1p(\n",
        "    pd.to_numeric(model_df['densidad_poblacional_2022'], errors='coerce').clip(lower=0)\n",
        ")\n",
        "\n",
        "feature_cols = [\n",
        "    'superficie_km2',\n",
        "    'shape_length',\n",
        "    'latitud',\n",
        "    'longitud',\n",
        "    'poblacion_2022',\n",
        "    'hogares_2022',\n",
        "    'viviendas_2022',\n",
        "    'edad_promedio_2022',\n",
        "    'pct_mujeres_2022',\n",
        "    'pct_urbana_2022',\n",
        "    'densidad_poblacional_2022',\n",
        "    'personas_por_hogar_2022',\n",
        "    'viviendas_por_km2_2022',\n",
        "    'indice_compacidad',\n",
        "    'log_densidad_poblacional_2022',\n",
        "]\n",
        "\n",
        "labeled_df = model_df[model_df['target_alto_riesgo'].notna()].copy()\n",
        "unlabeled_df = model_df[model_df['target_alto_riesgo'].isna()].copy()\n",
        "\n",
        "X = labeled_df[feature_cols]\n",
        "y = labeled_df['target_alto_riesgo'].astype(int)\n",
        "\n",
        "print('Shape X:', X.shape)\n",
        "print('Distribucion target:')\n",
        "print(y.value_counts().sort_index())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8) Entrenamiento de modelos requeridos\n",
        "\n",
        "Se entrena:\n",
        "\n",
        "1. Regresion Logistica (base)\n",
        "2. Arbol de Decision\n",
        "3. Ensamble (RL + DT + RF)\n",
        "4. Arbol optimizado con GridSearchCV (priorizando Recall)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X,\n",
        "    y,\n",
        "    test_size=0.25,\n",
        "    random_state=42,\n",
        "    stratify=y,\n",
        ")\n",
        "\n",
        "pre_scaled = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\n",
        "            'num',\n",
        "            Pipeline(\n",
        "                steps=[\n",
        "                    ('imputer', SimpleImputer(strategy='median')),\n",
        "                    ('scaler', StandardScaler()),\n",
        "                ]\n",
        "            ),\n",
        "            feature_cols,\n",
        "        )\n",
        "    ]\n",
        ")\n",
        "\n",
        "pre_unscaled = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\n",
        "            'num',\n",
        "            Pipeline(steps=[('imputer', SimpleImputer(strategy='median'))]),\n",
        "            feature_cols,\n",
        "        )\n",
        "    ]\n",
        ")\n",
        "\n",
        "model_lr = Pipeline(\n",
        "    steps=[\n",
        "        ('preprocess', pre_scaled),\n",
        "        ('clf', LogisticRegression(max_iter=2000, class_weight='balanced', random_state=42)),\n",
        "    ]\n",
        ")\n",
        "\n",
        "model_dt = Pipeline(\n",
        "    steps=[\n",
        "        ('preprocess', pre_unscaled),\n",
        "        ('clf', DecisionTreeClassifier(random_state=42, class_weight='balanced', min_samples_leaf=2)),\n",
        "    ]\n",
        ")\n",
        "\n",
        "model_rf = Pipeline(\n",
        "    steps=[\n",
        "        ('preprocess', pre_unscaled),\n",
        "        (\n",
        "            'clf',\n",
        "            RandomForestClassifier(\n",
        "                n_estimators=300,\n",
        "                random_state=42,\n",
        "                class_weight='balanced_subsample',\n",
        "                min_samples_leaf=2,\n",
        "            ),\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "\n",
        "model_ens = VotingClassifier(\n",
        "    estimators=[('lr', model_lr), ('dt', model_dt), ('rf', model_rf)],\n",
        "    voting='soft',\n",
        ")\n",
        "\n",
        "model_lr.fit(X_train, y_train)\n",
        "model_dt.fit(X_train, y_train)\n",
        "model_ens.fit(X_train, y_train)\n",
        "\n",
        "grid_dt = GridSearchCV(\n",
        "    estimator=model_dt,\n",
        "    param_grid={\n",
        "        'clf__criterion': ['gini', 'entropy'],\n",
        "        'clf__max_depth': [3, 5, 8, None],\n",
        "        'clf__min_samples_split': [2, 5, 10],\n",
        "        'clf__min_samples_leaf': [1, 2, 4],\n",
        "    },\n",
        "    scoring='recall',\n",
        "    cv=5,\n",
        "    n_jobs=1,\n",
        "    refit=True,\n",
        ")\n",
        "\n",
        "grid_dt.fit(X_train, y_train)\n",
        "model_dt_opt = grid_dt.best_estimator_\n",
        "\n",
        "print('Mejores hiperparametros (DT):', grid_dt.best_params_)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9) Evaluacion comparativa de modelos\n",
        "\n",
        "Metricas reportadas:\n",
        "\n",
        "- Precision\n",
        "- Recall (metrica prioritaria)\n",
        "- F1-score\n",
        "- ROC-AUC\n",
        "\n",
        "Criterio de seleccion final del notebook:\n",
        "\n",
        "1. mayor Recall,\n",
        "2. desempate con F1,\n",
        "3. desempate con ROC-AUC.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def eval_model(name: str, model, x_test: pd.DataFrame, y_test: pd.Series) -> dict:\n",
        "    y_pred = model.predict(x_test)\n",
        "    y_score = model.predict_proba(x_test)[:, 1]\n",
        "    return {\n",
        "        'modelo': name,\n",
        "        'precision': precision_score(y_test, y_pred, zero_division=0),\n",
        "        'recall': recall_score(y_test, y_pred, zero_division=0),\n",
        "        'f1': f1_score(y_test, y_pred, zero_division=0),\n",
        "        'roc_auc': roc_auc_score(y_test, y_score),\n",
        "        'y_pred': y_pred,\n",
        "        'y_score': y_score,\n",
        "    }\n",
        "\n",
        "results = [\n",
        "    eval_model('Regresion Logistica (Base)', model_lr, X_test, y_test),\n",
        "    eval_model('Arbol de Decision', model_dt, X_test, y_test),\n",
        "    eval_model('Ensamble RL+DT+RF', model_ens, X_test, y_test),\n",
        "    eval_model('Arbol de Decision Optimizado (GridSearchCV)', model_dt_opt, X_test, y_test),\n",
        "]\n",
        "\n",
        "metrics_table = pd.DataFrame([\n",
        "    {\n",
        "        'modelo': r['modelo'],\n",
        "        'precision': round(r['precision'], 4),\n",
        "        'recall': round(r['recall'], 4),\n",
        "        'f1': round(r['f1'], 4),\n",
        "        'roc_auc': round(r['roc_auc'], 4),\n",
        "    }\n",
        "    for r in results\n",
        "]).sort_values(['recall', 'f1', 'roc_auc'], ascending=False)\n",
        "\n",
        "metrics_table\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_model_name = metrics_table.iloc[0]['modelo']\n",
        "best_result = next(r for r in results if r['modelo'] == best_model_name)\n",
        "\n",
        "print('Modelo seleccionado (criterio recall):', best_model_name)\n",
        "print()\n",
        "print('Reporte de clasificacion:')\n",
        "print(classification_report(y_test, best_result['y_pred'], digits=4))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 6))\n",
        "for r in results:\n",
        "    RocCurveDisplay.from_predictions(y_test, r['y_score'], name=r['modelo'])\n",
        "\n",
        "plt.title('Curvas ROC - Comparacion de modelos')\n",
        "plt.grid(alpha=0.25)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10) Prediccion de parroquias sin historial y analisis de salida\n",
        "\n",
        "Se aplica el mejor modelo para completar parroquias que no tenian historial etiquetado,\n",
        "sin usar datos sinteticos.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_model = {\n",
        "    'Regresion Logistica (Base)': model_lr,\n",
        "    'Arbol de Decision': model_dt,\n",
        "    'Ensamble RL+DT+RF': model_ens,\n",
        "    'Arbol de Decision Optimizado (GridSearchCV)': model_dt_opt,\n",
        "}[best_model_name]\n",
        "\n",
        "all_probs = best_model.predict_proba(model_df[feature_cols])[:, 1]\n",
        "model_df['probabilidad_reestimada'] = all_probs\n",
        "\n",
        "q_low = float(model_df['probabilidad_reestimada'].quantile(0.33))\n",
        "q_high = float(model_df['probabilidad_reestimada'].quantile(0.66))\n",
        "\n",
        "\n",
        "def classify_risk(prob: float, low_threshold: float, high_threshold: float) -> str:\n",
        "    if prob >= high_threshold:\n",
        "        return 'Alto'\n",
        "    if prob >= low_threshold:\n",
        "        return 'Medio'\n",
        "    return 'Bajo'\n",
        "\n",
        "\n",
        "model_df['riesgo_reestimado'] = model_df['probabilidad_reestimada'].apply(\n",
        "    lambda p: classify_risk(float(p), q_low, q_high)\n",
        ")\n",
        "\n",
        "print('Distribucion de riesgo reestimado:')\n",
        "print(model_df['riesgo_reestimado'].value_counts())\n",
        "print()\n",
        "print('Parroquias originalmente sin historial:', len(unlabeled_df))\n",
        "\n",
        "model_df.loc[\n",
        "    model_df['target_alto_riesgo'].isna(),\n",
        "    ['codigo', 'parroquia', 'probabilidad_reestimada', 'riesgo_reestimado']\n",
        "].head(10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11) Coherencia con la aplicacion web (Flask + Leaflet)\n",
        "\n",
        "Se verifica que los artefactos consumidos por la web esten sincronizados:\n",
        "\n",
        "- `app/data/predicciones_parroquias.csv`\n",
        "- `app/data/parroquias_riesgo.geojson`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "app_pred_path = APP_DATA / 'predicciones_parroquias.csv'\n",
        "app_geojson_path = APP_DATA / 'parroquias_riesgo.geojson'\n",
        "\n",
        "app_pred_df = pd.read_csv(app_pred_path, dtype={'codigo': str})\n",
        "geo = json.loads(app_geojson_path.read_text(encoding='utf-8'))\n",
        "\n",
        "guayas_features = [\n",
        "    f for f in geo.get('features', [])\n",
        "    if str(f.get('properties', {}).get('provincia', '')).strip().upper() == 'GUAYAS'\n",
        "]\n",
        "\n",
        "missing_geo_risk = sum(1 for f in guayas_features if not f.get('properties', {}).get('riesgo'))\n",
        "missing_geo_prob = sum(\n",
        "    1 for f in guayas_features\n",
        "    if f.get('properties', {}).get('probabilidad') in (None, '', 'NaN')\n",
        ")\n",
        "\n",
        "print('CSV app - filas:', len(app_pred_df))\n",
        "print('GeoJSON - features Guayas:', len(guayas_features))\n",
        "print('GeoJSON faltantes riesgo:', missing_geo_risk)\n",
        "print('GeoJSON faltantes probabilidad:', missing_geo_prob)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12) Conclusiones tecnicas\n",
        "\n",
        "- Se cumple el flujo supervisado exigido: RL, DT, ensamble y DT optimizado con GridSearchCV.\n",
        "- La etiqueta objetivo fue construida tecnicamente desde historial de inundacion (sin etiquetas predefinidas).\n",
        "- Se prioriza Recall por contexto de gestion de riesgo (evitar falsos negativos).\n",
        "- La cobertura final incluye todas las parroquias oficiales de Guayas y completa probabilidades/riesgos faltantes.\n",
        "- La salida queda integrada al mapa web productivo mediante CSV + GeoJSON.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}